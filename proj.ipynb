{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3a52d395",
      "metadata": {},
      "source": [
        "# Projeto de Engenharia do Conhecimento 2023/2024\n",
        "\n",
        "*Projeto by: Renato Ferreira (58238), Pedro Lopes(58196), Simão Quintas (58190)*\n",
        "\n",
        "### Index\n",
        "\n",
        "1. Feature selection\n",
        "    1. Using correlation\n",
        "    2. Using stepwise methods\n",
        "    3. Random Forests for Feature Selection\n",
        "2. Principal Components analysis\n",
        "    1. Linear PCA\n",
        "    2. Kernel PCA\n",
        "3. Model Tuning\n",
        "\n",
        "\n",
        "## 1. Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ac6c13",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.11.0rc1 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d499b37",
      "metadata": {},
      "source": [
        "Start the imputer and get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bdfd5b79",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['age:', 'sex:', 'on thyroxine:', 'query on thyroxine:',\n",
            "       'on antithyroid medication:', 'sick:', 'pregnant:', 'thyroid surgery:',\n",
            "       'I131 treatment:', 'query hypothyroid:', 'query hyperthyroid:',\n",
            "       'lithium:', 'goitre:', 'tumor:', 'hypopituitary:', 'psych:',\n",
            "       'TSH measured:', 'TSH:', 'T3 measured:', 'T3:', 'TT4 measured:', 'TT4:',\n",
            "       'T4U measured:', 'T4U:', 'FTI measured:', 'FTI:', 'TBG measured:',\n",
            "       'TBG:', 'referral source:', 'diagnoses', '[record identification]'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
        "\n",
        "data = pd.read_csv('proj-data.csv', na_values='?') #, na_values='?' crasha o bloco seguinte\n",
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a21edad",
      "metadata": {},
      "source": [
        "Let's drop the rows and columns with a large number of NA values and get dummies for the columns with strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e6d5ffb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remover as linhas com pouca informação\n",
        "data.dropna(thresh=3, subset=['T3 measured:', 'TT4 measured:', 'T4U measured:', 'FTI measured:', 'TBG measured:'], inplace=True)\n",
        "\n",
        "# Remover as colunas com pouca informação\n",
        "data.dropna(axis=1, thresh=3669, inplace=True)\n",
        "\n",
        "# Remover as colunas que indicam se algo foi medido ou não e a que tem a indentificação\n",
        "columns_to_drop = data.filter(like='measured').columns\n",
        "data.drop(columns_to_drop, axis=1, inplace=True)\n",
        "data.drop('[record identification]', axis=1, inplace=True)\n",
        "\n",
        "# Remover linhas com homens grávidos\n",
        "data = data[~((data['sex:'] == 'M') & (data['pregnant:'] == \"t\"))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "eb844559",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      age: sex: on thyroxine: query on thyroxine: on antithyroid medication:  \\\n",
            "0       29    F             f                   f                          f   \n",
            "1       29    F             f                   f                          f   \n",
            "2       36    F             f                   f                          f   \n",
            "3       60    F             f                   f                          f   \n",
            "4       77    F             f                   f                          f   \n",
            "...    ...  ...           ...                 ...                        ...   \n",
            "7333    56    M             f                   f                          f   \n",
            "7334    22    M             f                   f                          f   \n",
            "7335    69    M             f                   f                          f   \n",
            "7336    47    F             f                   f                          f   \n",
            "7337    31    M             f                   f                          f   \n",
            "\n",
            "     sick: pregnant: thyroid surgery: I131 treatment: query hypothyroid:  ...  \\\n",
            "0        f         f                f               f                  t  ...   \n",
            "1        f         f                f               f                  f  ...   \n",
            "2        f         f                f               f                  f  ...   \n",
            "3        f         f                f               f                  f  ...   \n",
            "4        f         f                f               f                  f  ...   \n",
            "...    ...       ...              ...             ...                ...  ...   \n",
            "7333     f         f                f               f                  f  ...   \n",
            "7334     f         f                f               f                  f  ...   \n",
            "7335     f         f                f               f                  f  ...   \n",
            "7336     f         f                f               f                  f  ...   \n",
            "7337     f         f                f               f                  t  ...   \n",
            "\n",
            "     tumor: hypopituitary: psych: TSH:  T3:   TT4:  T4U:  FTI:  \\\n",
            "0         f              f      f  0.3  NaN    NaN   NaN   NaN   \n",
            "1         f              f      f  1.6  1.9  128.0   NaN   NaN   \n",
            "2         f              f      f  NaN  NaN    NaN   NaN   NaN   \n",
            "3         f              f      f  NaN  NaN    NaN   NaN   NaN   \n",
            "4         f              f      f  NaN  NaN    NaN   NaN   NaN   \n",
            "...     ...            ...    ...  ...  ...    ...   ...   ...   \n",
            "7333      f              f      f  NaN  NaN   64.0  0.83  77.0   \n",
            "7334      f              f      f  NaN  NaN   91.0  0.92  99.0   \n",
            "7335      f              f      f  NaN  NaN  113.0  1.27  89.0   \n",
            "7336      f              f      f  NaN  NaN   75.0  0.85  88.0   \n",
            "7337      f              f      f  NaN  NaN   66.0  1.02  65.0   \n",
            "\n",
            "      referral source:                  diagnoses  \n",
            "0                other             diagnosed none  \n",
            "1                other             diagnosed none  \n",
            "2                other             diagnosed none  \n",
            "3                other             diagnosed none  \n",
            "4                other             diagnosed none  \n",
            "...                ...                        ...  \n",
            "7333               SVI             diagnosed none  \n",
            "7334               SVI             diagnosed none  \n",
            "7335               SVI  diagnosed binding protein  \n",
            "7336             other             diagnosed none  \n",
            "7337             other             diagnosed none  \n",
            "\n",
            "[7467 rows x 23 columns]\n"
          ]
        }
      ],
      "source": [
        "def transform_diagnoses(df):\n",
        "\n",
        "    hyperthyroid_conditions = ['A', 'B', 'C', 'D']\n",
        "    hypothyroid_conditions = ['E', 'F', 'G', 'H']\n",
        "    binding_protein = ['I', 'J']\n",
        "    general_health = ['K']\n",
        "    replacement_therapy = ['L', 'M', 'N']\n",
        "    discordant = ['R']\n",
        "    other = ['O', 'P', 'Q', 'S', 'T']\n",
        "    none = ['-']\n",
        "\n",
        "    def replace_diagnoses(diagnoses_list):\n",
        "        replaced_diagnoses = []\n",
        "        for diag in diagnoses_list:\n",
        "            if diag in hyperthyroid_conditions:\n",
        "                replaced_diagnoses.append('diagnosed hyperthyroid')\n",
        "            elif diag in hypothyroid_conditions:\n",
        "                replaced_diagnoses.append('diagnosed hypothyroid')\n",
        "            elif diag in binding_protein:\n",
        "                replaced_diagnoses.append('diagnosed binding protein')\n",
        "            elif diag in general_health:\n",
        "                replaced_diagnoses.append('diagnosed general health')\n",
        "            elif diag in replacement_therapy:\n",
        "                replaced_diagnoses.append('diagnosed replacement therapy')\n",
        "            elif diag in discordant:\n",
        "                replaced_diagnoses.append('diagnosed antithyroid treatment')\n",
        "            elif diag in other:\n",
        "                replaced_diagnoses.append('diagnosed other')\n",
        "            elif diag in none:\n",
        "                replaced_diagnoses.append('diagnosed none')\n",
        "            else:\n",
        "                replaced_diagnoses.append('diagnosed other')\n",
        "\n",
        "        return replaced_diagnoses\n",
        "\n",
        "    df['diagnoses'] = df['diagnoses'].apply(lambda x: x.split(\"|\") if \"|\" in x else [x])\n",
        "    df['diagnoses'] = df['diagnoses'].apply(replace_diagnoses)\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# Encode categorical variables\n",
        "data = transform_diagnoses(data).explode('diagnoses')\n",
        "print(data)\n",
        "binary_cols = ['sex:', 'on thyroxine:', 'query on thyroxine:', 'on antithyroid medication:', \n",
        "               'sick:', 'pregnant:', 'thyroid surgery:', 'I131 treatment:', 'query hypothyroid:',\n",
        "               'query hyperthyroid:', 'lithium:', 'goitre:', 'tumor:', 'hypopituitary:', 'psych:',\n",
        "               'referral source:', 'diagnoses']\n",
        "\n",
        "# Trocar os t e f e os diagnosticos para valores\n",
        "data = pd.get_dummies(data, columns=binary_cols)\n",
        "\n",
        "imputed_data = imputer.fit_transform(data)\n",
        "\n",
        "df = pd.DataFrame(imputed_data, columns=data.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aabc254",
      "metadata": {},
      "source": [
        "Let's make a simple evaluation function running 2 regression algorithms and producing the R2 for each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5d5e9959",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 Decision Tree Regression:  0.4913\n",
            "R2 Linear Regression:  0.1730\n"
          ]
        }
      ],
      "source": [
        "# train-test split\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=0)\n",
        "\n",
        "def naive_model_testing(train, test):\n",
        "    diagnoses_cols = [col for col in train.columns if 'diagnoses' in col]\n",
        "    \n",
        "    #test 2 models, DTs and LR, and print out the results\n",
        "    dtr= DecisionTreeRegressor(max_depth=5)\n",
        "    dtr.fit(train.drop(diagnoses_cols, axis=1), train[diagnoses_cols])\n",
        "\n",
        "    lmr=LinearRegression()\n",
        "    lmr.fit(train.drop(diagnoses_cols, axis=1), train[diagnoses_cols])\n",
        "\n",
        "   # rf_preds=rfr.predict(X_test)\n",
        "    dt_preds=dtr.predict(test.drop(diagnoses_cols, axis=1))\n",
        "    lr_preds=lmr.predict(test.drop(diagnoses_cols, axis=1))\n",
        "\n",
        "   # print(\"RVE RFs: %7.4f\" % explained_variance_score(y_test, rf_preds))\n",
        "    print(\"R2 Decision Tree Regression: %7.4f\" % r2_score(test[diagnoses_cols], dt_preds))\n",
        "    print(\"R2 Linear Regression: %7.4f\" % r2_score(test[diagnoses_cols], lr_preds))\n",
        "\n",
        "naive_model_testing(train, test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a03b43b",
      "metadata": {},
      "source": [
        "### Correlation \n",
        "\n",
        "As a first exercise we are going to use the Spearman correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "948d42ab",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "spear = df.corr(method='spearman')\n",
        "spear"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e903bff",
      "metadata": {},
      "source": [
        "## 2. Principal Components Analysis\n",
        "\n",
        "We are now going to use the [PCA module](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) from scikit-learn\n",
        "\n",
        "First let's just find a 2D projection of our data (remember to use only the training set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9161accd",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2).set_output(transform=\"pandas\") #finding the two best PCs\n",
        "pca.fit(train.drop('target', axis=1))\n",
        "tve=0 #total variance explained\n",
        "for i, ve in enumerate(pca.explained_variance_ratio_):\n",
        "    tve+=ve\n",
        "    print(\"PC%d - Variance explained: %7.4f - Total Variance: %7.4f\" % (i, ve, tve) )\n",
        "print()\n",
        "print(\"Actual Eigenvalues:\", pca.singular_values_)\n",
        "for i,comp in enumerate(pca.components_):\n",
        "    print(\"PC\",i, \"-->\", comp)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "315d51e5",
      "metadata": {},
      "source": [
        "### Exercise 3\n",
        "\n",
        "1. Interpret the results above. \n",
        "   \n",
        "2. What is the meaning of the PC vectors?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2ec5770",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3.1 \n",
        "# A variância explicada de PC0 é maior, ou seja, este componente captura uma proporção significativamente maior da variabilidade total presente nos dados em comparação com os outros componentes.\n",
        "# Actual Eigenvalues representam o tamanho dos principais vetores de componentes, mostrando que PC0 tem uma maior importância na variância.\n",
        "# Em PC0 e PC1, as variáveis com maior valor absoluto contribuem mais para a definição do respetivo componente.\n",
        "\n",
        "# Exercise 3.2 \n",
        "# Os PC vectors representam as direções no espaço de características original ao longo das quais os dados variam mais. \n",
        "# Cada PC vector é uma combinação linear das variáveis originais, indicando quanto cada variável original contribui para essa direção ou eixo específico."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c02458",
      "metadata": {},
      "source": [
        "Now let's project the data using the principal components defined and use them for regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3f1c2d9",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "n_train=pca.transform(train.drop('target', axis=1))\n",
        "n_test=pca.transform(test.drop('target', axis=1))\n",
        "n_train['target'] = train['target']  # form the train dataframe to pass to the metrics function\n",
        "n_test['target'] = test['target']  # form the test dataframe to pass to the metrics function\n",
        "naive_model_testing(n_train, n_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98a8bfbf",
      "metadata": {},
      "source": [
        "quite poor results as expected"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3350cf10",
      "metadata": {},
      "source": [
        "### A graphical view illustrated with binary classification data\n",
        "\n",
        "We consider now the same data as a classification problem, assuming that patients with a target value of 250 or more means they have diabetes and with less that 250 they don't"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a8365e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# with binary classified instances\n",
        "# target values of 250 or more indicate diabetes\n",
        "yc_diabetes=np.array([int(i>=250) for i in diabetes.target]) # to be used in graphics ahead\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, yc_diabetes, test_size=0.2, random_state=23)\n",
        "print(\"training set patients with target value >=250: \", (y_train).sum())\n",
        "print(\"training set patients with target value <250: \", len(y_train) - (y_train).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa650325",
      "metadata": {},
      "source": [
        "Let's plot the projection in 2 components "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f262b866",
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=2) #finding the two best PCs\n",
        "pca.fit(X_train)\n",
        "nX_train=pca.transform(X_train)\n",
        "nX_test=pca.transform(X_test)\n",
        "colors=np.array([\"tab:blue\", \"tab:orange\"])[y_train]\n",
        "plt.scatter(nX_train[:,0], nX_train[:,1], c=colors)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6c0cdb",
      "metadata": {},
      "source": [
        "also as a classification problem we can see it is hard to discriminate the two classes using only the two PCs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fce3d12",
      "metadata": {},
      "source": [
        "## 3. Model Tuning\n",
        "\n",
        "For this example we are going to use Support Vector Classifiers, but any model learned so far can be used\n",
        "\n",
        "We are going to use first [Scikit-Learn's GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), an implementation of extensive parameter search. In its basic form it just requires:\n",
        "* a bare bones model constructor \n",
        "* a dictionary containing the parameters to search for. The keys of the dictionary should correspond to the parameter to test and the values to a list of possible values to test\n",
        "* a scoring function defining what is the criterion to select and rank the best models\n",
        "* GridSearchCV uses by default 5-Fold Cross validation, but other validation criteria can be used\n",
        "\n",
        "The result of GridSearchCV is a structure that contains the fitted models that can then be used for learning and application\n",
        "\n",
        "Tet's try it with the C and gamma values for support vector classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90792ad8-2eab-42db-bb74-f7809c2ae07e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "#from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "import scipy.stats as stats\n",
        "\n",
        "#make the dictionary with the testing parameters\n",
        "#gammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
        "#Cs = [1, 10, 100, 1e3, 1e4, 1e5]\n",
        "#param_grid = {'gamma': gammas, 'C': Cs}\n",
        "depths = [3, 5, 10, 15]\n",
        "m_sampl_split = [2, 5, 9]\n",
        "prune_a = [0.0, 0.0001, 0.001, 0.01]\n",
        "param_grid = {'max_depth': depths, 'min_samples_split': m_sampl_split, 'ccp_alpha': prune_a}\n",
        "\n",
        "#define the model and do the grid search\n",
        "#clf = SVC() # RBF (Gaussian) by default\n",
        "clf = DecisionTreeClassifier(criterion='log_loss', random_state=23)\n",
        "gs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=\"f1\")\n",
        "\n",
        "start = time()\n",
        "gs=gs.fit(X_train, y_train)\n",
        "print(\n",
        "    'GridSearchCV took %.2f seconds for %d candidate parameter settings.'\n",
        "    % ((time() - start), len(gs.cv_results_['params']))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05c22db8-554e-4401-8d21-be9a209c7257",
      "metadata": {},
      "source": [
        "Let's identify the best element parameters [best according to the scoring function, in this case it is the F1 score]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c028485-0116-4afa-bffa-fb492a77f370",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#print('best gamma: %7.4f' % gs.best_estimator_.gamma)\n",
        "#print('best C: %3.2f' %  gs.best_estimator_.C)\n",
        "print('best maximum depth: %2.0f' % gs.best_estimator_.max_depth)\n",
        "print('best minimum samples to split a node: %2.0f' %  gs.best_estimator_.min_samples_split)\n",
        "print('best minimal cost pruning parameter: %1.4f' % gs.best_estimator_.ccp_alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d12bb095-23ca-4436-bab1-398ad91fd5a7",
      "metadata": {},
      "source": [
        "Just for sake of completion, we can use the best estimator model (the one with the optimized parameters) for prediction on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7f10d58-1b86-427a-8a82-0759a8e67d1e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "preds=gs.best_estimator_.predict(X_test)\n",
        "print('F1 : %7.4f' % f1_score(y_test, preds))\n",
        "print('number of leaves:', gs.best_estimator_.get_n_leaves())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d40c323",
      "metadata": {},
      "source": [
        "GridSearchCV gives you a number of statistics on the tests it runs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3156ce1-c771-4a5f-88e3-aab94117af81",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for i in gs.cv_results_.keys(): print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b248aac8-6e6e-4851-aa7b-4af3f8e2637e",
      "metadata": {},
      "source": [
        "We can print the results in a nice Pandas Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7031e823-bfe7-4ff2-b96f-965f2a35df43",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "grid_res = pd.DataFrame(gs.cv_results_)\n",
        "grid_res.sort_values(by=['rank_test_score'], ascending=True, inplace=True) #sort the tested models by score\n",
        "grid_res[['params', 'rank_test_score', 'mean_test_score', 'std_test_score', 'mean_fit_time', 'std_fit_time']] #show only mean and std of the test score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13451945",
      "metadata": {},
      "source": [
        "we can check if the 2nd best model produces different results "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd1e9d44",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print('max_depth:', grid_res.loc[1, 'param_max_depth'],\n",
        "      'min_samples_split:', grid_res.loc[1, 'param_min_samples_split'],\n",
        "      'ccp_alpha:', '{:.2e}'.format(grid_res.loc[1, 'param_ccp_alpha']))\n",
        "clf = DecisionTreeClassifier(criterion='log_loss', random_state=23,\n",
        "                             max_depth=grid_res.loc[1, 'param_max_depth'],\n",
        "                             min_samples_split=grid_res.loc[1, 'param_min_samples_split'],\n",
        "                             ccp_alpha=grid_res.loc[1, 'param_ccp_alpha'])\n",
        "clf.fit(X_train, y_train)\n",
        "preds=clf.predict(X_test)\n",
        "print('F1 : %7.4f' % f1_score(y_test, preds))\n",
        "print('number of leaves:', clf.get_n_leaves())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f46b39f",
      "metadata": {},
      "source": [
        "Let's try now the RandomizedSearchCV and compare to the previous one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e298a0e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# configure randomized search (by default also 5-fold CV)\n",
        "# notice the loguniform distributions\n",
        "\n",
        "param_dist = {\n",
        "#    'C': stats.loguniform(1, 1e5),\n",
        "#    'gamma': stats.loguniform(1e-7, 1e-1),\n",
        "    'max_depth': stats.randint(3, 16),\n",
        "    'min_samples_split': stats.randint(2, 10),\n",
        "    'ccp_alpha': stats.loguniform(1e-5, 0.01)\n",
        "}\n",
        "\n",
        "n_iter_search = 15\n",
        "rs = RandomizedSearchCV(\n",
        "    clf, param_distributions=param_dist, n_iter=n_iter_search\n",
        ")\n",
        "\n",
        "start = time()\n",
        "rs = rs.fit(X_train, y_train)\n",
        "print(\n",
        "    'RandomizedSearchCV took %.2f seconds for %d candidates parameter settings'\n",
        "    % ((time() - start), n_iter_search)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b307be",
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print('best maximum depth: %2.0f' % rs.best_estimator_.max_depth)\n",
        "print('best minimum samples to split a node: %2.0f' %  rs.best_estimator_.min_samples_split)\n",
        "print('best minimal cost pruning parameter: %1.4f' % rs.best_estimator_.ccp_alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a1243f8",
      "metadata": {},
      "source": [
        "Now we can use the best estimator model (the one with the optimized parameters) for prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be64837f",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "rs1 = rs.best_estimator_\n",
        "rs1.fit(X_train, y_train)\n",
        "preds=rs1.predict(X_test)\n",
        "print('F1 : %7.4f' % f1_score(y_test, preds))\n",
        "print('number of leaves:', rs1.get_n_leaves())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7931e79e",
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "rand_res = pd.DataFrame(rs.cv_results_)\n",
        "rand_res.sort_values(by=['rank_test_score'], ascending= True, inplace=True) #sort the tested models by score\n",
        "rand_res[['params', 'rank_test_score', 'mean_test_score', 'std_test_score', 'mean_fit_time', 'std_fit_time']] #show only mean and std of the test score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "579f7035",
      "metadata": {},
      "source": [
        "checking the 2nd best model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55d1bd4e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print('max_depth:', rand_res['param_max_depth'].iat[1],\n",
        "      ', min_samples_split:', rand_res['param_min_samples_split'].iat[1],\n",
        "      ', ccp_alpha:', '{:.2e}'.format(rand_res['param_ccp_alpha'].iat[1]))\n",
        "clf = DecisionTreeClassifier(criterion='log_loss', random_state=23,\n",
        "                             max_depth=rand_res['param_max_depth'].iat[1],\n",
        "                             min_samples_split=rand_res['param_min_samples_split'].iat[1],\n",
        "                             ccp_alpha=rand_res['param_ccp_alpha'].iat[1])\n",
        "clf.fit(X_train, y_train)\n",
        "preds=clf.predict(X_test)\n",
        "print('F1 : %7.4f' % f1_score(y_test, preds))\n",
        "print('number of leaves:', clf.get_n_leaves())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "050922b9-b615-4eac-9ae3-79badb08de2c",
      "metadata": {},
      "source": [
        "### Exercise 4\n",
        "1. Discuss the values above in terms of coherency of the parameters found. Do you find a pattern in the best values for max_dept and ccp_alpha?\n",
        "2. Compare the first 3 models results using the testing set and discuss your findings [Optional]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a64dd64-a8c9-48be-92d5-14b6db9fcea5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4.1\n",
        "#Enquanto que o valor para o max_dept parece ser consistentemente 3, o valor para ccp_aplpha vaira bastante\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d9dd32d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4.2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1bab531",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comments on results of Exercise 4.2\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11.0rc1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
